# Story 4.25-1: Unified API Client (Vercel SDK)

Status: in-progress

<!-- Note: Validation is optional. Run validate-create-story for quality check before dev-story. -->

## Story

As a **developer**,
I want **a unified AI API client that supports multiple providers (OpenRouter, OpenAI, Anthropic, Google, etc.) through the Vercel AI SDK**,
So that **I can easily switch between AI providers without rewriting API integration logic, enabling cost optimization and fallback strategies**.

## Acceptance Criteria

### 1. Vercel AI SDK Integration

**Given** the project needs multi-provider AI support
**When** implementing the unified client
**Then** install `@ai-sdk/provider-registry` npm package (v1.1.0+) for frontend type definitions
**And** install `ai` crate (v0.8.0+) in Rust backend for provider abstraction
**And** NO provider-specific SDKs installed (e.g., no `@anthropic-ai/sdk`, no `openai` package) to minimize bundle size
**And** provider configuration defined in TypeScript types:
```typescript
interface ProviderConfig {
  id: string; // 'openrouter' | 'openai' | 'anthropic' | 'google' | 'custom'
  name: string; // Human-readable name
  baseUrl: string; // API endpoint
  requiresKey: boolean; // Whether API key is required
  isDefault: boolean; // Only one can be true
}
```
**And** provider registry supports runtime extension (future: user-defined providers)

### 2. Provider Configuration Storage

**Given** the user has multiple AI provider credentials
**When** storing API keys
**Then** SQLite `settings` table is used with encrypted storage
**And** schema includes:
```sql
-- settings table (existing, extend for multi-provider)
INSERT INTO settings (key, value) VALUES
  ('ai_provider_default', 'openrouter'), -- Default provider ID
  ('api_key_openrouter', '{encrypted}'), -- OpenRouter API key
  ('api_key_openai', '{encrypted}'),     -- OpenAI API key (optional)
  ('api_key_anthropic', '{encrypted}');  -- Anthropic API key (optional)
```
**And** encryption uses `ring` crate AES-256-GCM (per Architecture doc Category 5)
**And** API keys are NEVER logged or exposed in error messages
**And** backend command `get_ai_providers() -> Result<Vec<ProviderInfo>, String>` returns list of configured providers with:
  - `id`, `name`, `is_configured` (bool - has API key), `is_default` (bool)
**And** backend command `set_default_provider(provider_id: String) -> Result<(), String>` updates default
**And** backend command `save_api_key(provider_id: String, api_key: String) -> Result<(), String>` encrypts and stores key

### 3. Unified Streaming Interface

**Given** the AI context generation uses streaming responses
**When** switching providers
**Then** the streaming interface remains consistent across all providers
**And** Rust backend abstracts provider-specific streaming logic:
```rust
pub trait AiProvider {
    async fn stream_context(
        &self,
        payload: ContextPayload,
    ) -> Result<Pin<Box<dyn Stream<Item = String> + Send>>, AiError>;
}
```
**And** each provider implements the trait (OpenRouterProvider, OpenAiProvider, etc.)
**And** frontend receives identical event format regardless of provider:
```typescript
interface AiChunkEvent {
  content: string; // Incremental chunk
  isComplete: boolean; // Final chunk flag
  provider: string; // Provider ID that served this response
}
```
**And** streaming uses Tauri event system: `window.emit("ai-chunk", event)`
**And** provider implementation follows Architecture doc IPC patterns (Category 1)

### 4. Provider Selection Logic

**Given** the user has configured multiple providers
**When** requesting AI context
**Then** the system uses the default provider specified in settings
**And** if API call fails with 401/403 (auth error), emit error event without retry
**And** if API call fails with 429 (rate limit), emit error with retry suggestion
**And** if API call fails with 500+ (server error), emit error without automatic fallback
**And** error events include provider ID:
```typescript
interface AiErrorEvent {
  provider: string;
  errorType: 'auth' | 'ratelimit' | 'server' | 'network';
  message: string; // Empathetic user-facing message (仁 Jin)
  retryable: boolean;
}
```
**And** frontend displays empathetic error messages per project-context.md (Philosophy Rules)
**And** user can manually switch provider in settings (no automatic fallback in MVP)

### 5. Backward Compatibility

**Given** existing OpenRouter integration (Story 3.1) is in production
**When** migrating to unified client
**Then** existing `settings` table entries for OpenRouter API key are preserved
**And** default provider is set to 'openrouter' on first migration
**And** existing AI context generation functionality continues to work without changes
**And** migration path documented in code comments:
```rust
// Migration: Existing 'api_key' setting becomes 'api_key_openrouter'
// Migration: Add 'ai_provider_default' = 'openrouter'
```
**And** **NO UI changes in this story** (Settings UI comes in Story 4.25-3)
**And** **NO breaking changes to existing Tauri commands** (`get_ai_context` signature unchanged)

## Tasks / Subtasks

- [ ] **Backend: Provider Abstraction Layer (src-tauri/src/ai/provider.rs)**
  - [ ] Define `AiProvider` trait with `stream_context` method
  - [ ] Define `AiError` enum with variants: Auth, RateLimit, Server, Network
  - [ ] Implement `ContextPayload` struct (already exists, ensure compatibility)
  - [ ] Add `ProviderInfo` struct for `get_ai_providers` command
  - [ ] **Error Handling:**
    - Return `AiError::Auth` for 401/403 status codes
    - Return `AiError::RateLimit` for 429 status codes
    - Return `AiError::Server` for 500+ status codes
    - Return `AiError::Network` for connection errors
    - Map errors to empathetic messages per Philosophy Rules (仁 Jin)

- [ ] **Backend: OpenRouter Provider Implementation (src-tauri/src/ai/providers/openrouter.rs)**
  - [ ] Migrate existing OpenRouter logic from `src-tauri/src/ai/client.rs` to provider pattern
  - [ ] Implement `AiProvider` trait for `OpenRouterProvider`
  - [ ] Use `reqwest` for HTTP streaming (existing dependency)
  - [ ] Parse SSE (Server-Sent Events) chunks into unified format
  - [ ] Handle OpenRouter-specific headers (`HTTP-Referer`, `X-Title`)
  - [ ] **Performance Target:** First chunk <2s, total <10s (NFR1)
  - [ ] Add unit tests with mocked HTTP responses

- [ ] **Backend: Settings Commands (src-tauri/src/commands/settings.rs)**
  - [ ] Implement `get_ai_providers() -> Result<Vec<ProviderInfo>, String>`
    - Query `settings` table for all `api_key_*` entries
    - Return provider list with `is_configured` flag
    - Mark default provider using `ai_provider_default` setting
  - [ ] Implement `set_default_provider(provider_id: String) -> Result<(), String>`
    - Validate provider_id exists in registry
    - Update `settings` table `ai_provider_default` value
  - [ ] Implement `save_api_key(provider_id: String, api_key: String) -> Result<(), String>`
    - Encrypt API key using `ring` crate (see Architecture doc)
    - Store as `api_key_{provider_id}` in `settings` table
    - Return error if encryption fails
  - [ ] Add tests for all commands

- [ ] **Backend: Migration Logic (src-tauri/src/db/migrations.rs)**
  - [ ] Create migration function `migrate_to_multi_provider()`
  - [ ] Check if `api_key` exists (old OpenRouter key)
  - [ ] If exists, rename to `api_key_openrouter` and add `ai_provider_default = 'openrouter'`
  - [ ] If not exists (fresh install), no migration needed
  - [ ] Add migration to initialization sequence in `src-tauri/src/db/mod.rs`
  - [ ] Test migration with existing database and fresh database

- [ ] **Backend: Unified Client (src-tauri/src/ai/mod.rs)**
  - [ ] Update `get_ai_context` command to use provider abstraction
  - [ ] Load default provider from settings
  - [ ] Instantiate provider implementation based on ID
  - [ ] Call `provider.stream_context(payload)` and emit events
  - [ ] Emit `AiChunkEvent` with provider ID field
  - [ ] Emit `AiErrorEvent` on failures with provider ID and error type
  - [ ] **Maintain backward compatibility:** Existing `ai-chunk` event format unchanged (add optional `provider` field)

- [ ] **Frontend: Types (src/types/ai.ts)**
  - [ ] Add `ProviderConfig` interface
  - [ ] Add `ProviderInfo` interface for command responses
  - [ ] Extend `AiChunkEvent` with optional `provider?: string` field (backward compatible)
  - [ ] Add `AiErrorEvent` interface matching backend

- [ ] **Frontend: Store (src/stores/aiStore.ts - NEW)**
  - [ ] Create store for AI provider state:
    ```typescript
    interface AiStore {
      providers: ProviderInfo[]; // Available providers
      defaultProvider: string | null; // Current default ID
      isLoading: boolean;
      error: string | null;

      loadProviders: () => Promise<void>;
      setDefaultProvider: (id: string) => Promise<void>;
      saveApiKey: (providerId: string, key: string) => Promise<void>;
    }
    ```
  - [ ] Implement actions using Tauri commands
  - [ ] Use Zustand for state management (per Architecture doc)
  - [ ] Add error handling with empathetic messages

- [ ] **Documentation**
  - [ ] Add inline comments documenting provider abstraction pattern
  - [ ] Document migration path from old OpenRouter setup
  - [ ] Add architecture decision rationale in code:
    ```rust
    // ADR: Use Vercel AI SDK abstraction for multi-provider support
    // Rationale: Enables cost optimization and provider switching without
    //            rewriting integration logic. Reduces vendor lock-in.
    ```
  - [ ] Update `docs/architecture.md` references if needed

- [ ] **Testing**
  - [ ] Unit test `AiProvider` trait with mocked provider
  - [ ] Unit test OpenRouter provider implementation
  - [ ] Unit test migration logic (existing key → multi-provider)
  - [ ] Integration test: Fresh install with default OpenRouter
  - [ ] Integration test: Existing install migration
  - [ ] Test: Invalid provider ID returns error
  - [ ] Test: Missing API key returns empathetic error
  - [ ] Test: Rate limit error (429) emits retryable event
  - [ ] Test: Auth error (401) emits non-retryable event
  - [ ] Test: Server error (500) emits error event
  - [ ] Performance test: Streaming first chunk <2s
  - [ ] Performance test: Total completion <10s
  - [ ] **Regression Tests:**
    - [ ] Story 3.1: OpenRouter integration still works
    - [ ] Story 3.4: AI context generation still works
    - [ ] Story 3.6: Error states display correctly
    - [ ] All existing AI-related tests pass

## Technical Requirements

### Performance Targets

| Metric | Target | Critical Path |
|--------|--------|---------------|
| First AI chunk | <2s | Local DB query + API call + first SSE event |
| Total AI response | <10s | Full streaming completion (NFR1) |
| Provider switch | <100ms | Settings update in SQLite |
| Migration | <500ms | One-time on app startup |

### Architecture Alignment

**Category 1: State Management**
- Frontend: Use Zustand for `aiStore` (consistent with existing pattern)
- Backend: Use Tauri managed state for provider registry
- IPC: Commands for queries, events for streaming

**Category 2: Context Aggregation**
- Provider abstraction does NOT change payload structure
- Existing <10KB limit maintained (NFR29)
- Attribution includes provider ID: "Based on: 15 edits (via OpenRouter)"

**Category 4: Git Operations**
- No git changes in this story (AI client only)

**Category 5: Performance**
- Provider instantiation cached in AppState (avoid re-creating on each call)
- API keys decrypted once per session, cached in memory (not re-read from SQLite)

### Security Considerations

**API Key Storage (NFR11):**
- Use `ring` crate for AES-256-GCM encryption
- Keys stored as encrypted blobs in SQLite
- Decryption key derived from system keyring (Linux: libsecret)
- Fallback: User password-based encryption if keyring unavailable

**Privacy (義 Gi - Righteous):**
- API keys NEVER logged
- API keys NEVER included in error messages
- Provider selection visible to user (transparent)

### Provider Registry (Extensible Design)

**Built-in Providers (MVP):**
```typescript
const PROVIDER_REGISTRY: ProviderConfig[] = [
  {
    id: 'openrouter',
    name: 'OpenRouter',
    baseUrl: 'https://openrouter.ai/api/v1',
    requiresKey: true,
    isDefault: true,
  },
  // Future providers (not implemented in this story):
  // { id: 'openai', name: 'OpenAI', baseUrl: 'https://api.openai.com/v1', requiresKey: true },
  // { id: 'anthropic', name: 'Anthropic', baseUrl: 'https://api.anthropic.com/v1', requiresKey: true },
  // { id: 'demo', name: 'Demo Mode (AWS Lambda)', baseUrl: 'https://...', requiresKey: false },
];
```

**Note:** This story only implements OpenRouter provider. Other providers added in future stories.

### Error Handling Strategy

**Error Categories:**
| Status Code | Error Type | Retryable | User Message |
|-------------|-----------|-----------|--------------|
| 401/403 | Auth | No | "API key invalid. Check your settings." |
| 429 | RateLimit | Yes | "AI resting. Try again in 30s." |
| 500-599 | Server | No | "AI service unavailable. Try again later?" |
| Network | Network | Yes | "Couldn't reach AI. Check your connection." |

**Philosophy Alignment (仁 Jin - Compassion):**
- All messages empathetic, not technical
- Suggest actionable fixes ("Check settings", "Try again")
- Never blame user ("You entered invalid key" → "API key invalid")

### Migration Strategy

**First Launch After Update:**
```rust
// In src-tauri/src/db/mod.rs initialization:
pub async fn initialize_db() -> Result<SqlitePool, DbError> {
    let pool = create_pool().await?;
    run_migrations(&pool).await?;
    migrate_to_multi_provider(&pool).await?; // NEW
    Ok(pool)
}
```

**Migration Logic:**
```rust
pub async fn migrate_to_multi_provider(pool: &SqlitePool) -> Result<(), DbError> {
    // Check if old 'api_key' exists
    let old_key = sqlx::query_as::<_, (String,)>("SELECT value FROM settings WHERE key = 'api_key'")
        .fetch_optional(pool)
        .await?;

    if let Some((key_value,)) = old_key {
        // Rename to 'api_key_openrouter'
        sqlx::query("INSERT INTO settings (key, value) VALUES ('api_key_openrouter', ?) ON CONFLICT(key) DO UPDATE SET value = ?")
            .bind(&key_value)
            .bind(&key_value)
            .execute(pool)
            .await?;

        // Set default provider
        sqlx::query("INSERT INTO settings (key, value) VALUES ('ai_provider_default', 'openrouter') ON CONFLICT(key) DO NOTHING")
            .execute(pool)
            .await?;

        // Delete old key
        sqlx::query("DELETE FROM settings WHERE key = 'api_key'")
            .execute(pool)
            .await?;
    }

    Ok(())
}
```

### UX/UI Details

**No UI Changes in This Story**
- Settings UI for provider selection comes in Story 4.25-3
- This story only implements backend infrastructure
- Existing OpenRouter-based AI context generation continues to work seamlessly

**Attribution Update (Minor):**
- Existing attribution: "Based on: 15 edits · 3 searches · DEVLOG"
- New attribution: "Based on: 15 edits · 3 searches · DEVLOG (via OpenRouter)"
- Implementation: Append `(via {provider.name})` to existing attribution string

## Dev Notes

### File Structure

```
src-tauri/src/ai/
├── mod.rs              # Unified client, get_ai_context command
├── provider.rs         # AiProvider trait, AiError enum
├── providers/
│   ├── mod.rs          # Provider registry
│   └── openrouter.rs   # OpenRouterProvider implementation
└── client.rs           # DEPRECATED (migrate logic to provider.rs)

src-tauri/src/commands/
└── settings.rs         # get_ai_providers, set_default_provider, save_api_key

src-tauri/src/db/
└── migrations.rs       # migrate_to_multi_provider()

src/stores/
└── aiStore.ts          # NEW (provider management state)

src/types/
└── ai.ts               # ProviderConfig, ProviderInfo, AiErrorEvent
```

### Dependencies

**Backend (Cargo.toml):**
```toml
[dependencies]
ai = "0.8.0"           # Vercel AI SDK Rust bindings (if available, else custom trait)
ring = "0.17"          # Encryption for API keys
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
reqwest = { version = "0.11", features = ["stream"] }
sqlx = { version = "0.7", features = ["sqlite", "runtime-tokio-native-tls"] }
```

**Frontend (package.json):**
```json
{
  "dependencies": {
    "@ai-sdk/provider-registry": "^1.1.0",
    "zustand": "^4.5.0"
  }
}
```

**Note:** If `ai` crate doesn't exist, implement custom `AiProvider` trait (recommended for MVP simplicity).

### Reference Documents

| File | Purpose | When to Read |
|------|---------|--------------|
| `docs/analysis/research/technical-epic-4.25-multi-provider-api-research-2025-12-22.md` | Vercel AI SDK research, provider abstraction patterns, cost analysis | Read before implementing provider trait (lines 1-150) |
| `docs/architecture.md` | Category 1 (State Management), Category 5 (Performance), API key encryption patterns | Read for Zustand patterns (lines 1068-1174), encryption (lines 1612-1615) |
| `docs/project-context.md` | Error handling philosophy (仁 Jin), empathetic messaging examples | Read for error message wording (lines 146-154) |
| `docs/epics.md` | Epic 4.25 full context, Story 4.25-1 BDD scenarios | Reference for acceptance criteria alignment |

### Critical Implementation Notes

1. **No Automatic Fallback:** If default provider fails, do NOT automatically try another provider. User must manually switch in settings (story 4.25-3).

2. **Streaming Consistency:** All providers MUST emit identical `AiChunkEvent` format. Provider-specific streaming quirks (SSE vs JSON stream) handled internally.

3. **Migration Safety:** Migration MUST be idempotent. Running twice should not duplicate entries or corrupt data.

4. **Performance Budget:** Provider instantiation cached in `AppState` to avoid re-creating HTTP client on each request. Measure memory impact.

5. **Error Transparency:** Error events include provider ID so user knows which provider failed (helps with debugging API key issues).

## Dev Agent Record

### Agent Model Used

{{agent_model_name_version}}

### Debug Log References

### Completion Notes List

### File List

<!-- To be filled during implementation -->
