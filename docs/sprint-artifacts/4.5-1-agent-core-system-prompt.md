# Story 4.5.1: Agent Core & System Prompt

Status: ready-for-dev

<!-- Validation completed: 2025-12-22 - All critical improvements applied -->

## Dependencies

**Requires (must be completed first):**
- Story 4.25.1: Unified API Client (backend `AiProvider` trait created, but NO Vercel SDK client yet)
- Story 3.4: AI Context Generation (existing streaming infrastructure)

**Key Dependency Note:**
- Story 4.25.1 created backend provider abstraction but **DID NOT** create `src/lib/ai/client.ts`
- This story creates the **FIRST** Vercel AI SDK runtime integration (not extending existing client)
- Backend currently supports only single `system_prompt + user_message` â†’ This story adds `messages` array support

**Blocks (cannot start until this is done):**
- Story 4.5.2: Tool Implementation & Protocol Execution (requires agent core and mock tools from this story)

## Story

As a **developer**,
I want **the agent core with unified system prompt, tool scaffolding, and Vercel AI SDK client**,
So that **the reasoning infrastructure can be tested with mock tools before building full tool implementations and UI**.

## Acceptance Criteria

### 1. Types & Interfaces

**Given** the reasoning infrastructure needs type safety
**When** defining core types
**Then** the following TypeScript types are created in `src/types/agent.ts`:
- [ ] `AgentMode` type: `"ronin-flash" | "ronin-thinking"`
- [ ] `ReasoningProtocol` interface with:
  ```typescript
  interface ReasoningProtocol {
    id: string;               // e.g., "project-resurrection"
    title: string;
    description: string;
    steps: ProtocolStep[];
  }
  ```
- [ ] `ProtocolStep` interface with:
  ```typescript
  interface ProtocolStep {
    id: string;               // e.g., "step-01-map-structure"
    title: string;
    instruction: string;      // LLM instruction for this step
    requiredOutput: 'file_creation' | 'user_confirmation' | 'none';
  }
  ```

### 2. Backend AI Infrastructure (Breaking Change - Messages Array)

**Given** reasoning loops require multi-turn conversation support
**When** upgrading the backend
**Then** the following changes are implemented:

**A. Message Struct Definition (src-tauri/src/ai/provider.rs):**
- [ ] `Message` struct defined:
  ```rust
  #[derive(Debug, Clone, Serialize, Deserialize)]
  pub struct Message {
      pub role: String,    // "system", "user", "assistant", "tool"
      pub content: String,
      #[serde(skip_serializing_if = "Option::is_none")]
      pub name: Option<String>, // For tool calls
  }
  ```

**B. Payload Update:**
- [ ] `ContextPayload` struct updated to support BOTH formats:
  ```rust
  #[derive(Debug, Clone, Serialize, Deserialize)]
  pub struct ContextPayload {
      // NEW: Multi-turn conversation support
      #[serde(skip_serializing_if = "Option::is_none")]
      pub messages: Option<Vec<Message>>,
      
      // LEGACY: Single-turn support (Story 3.4 compatibility)
      #[serde(skip_serializing_if = "Option::is_none")]
      pub system_prompt: Option<String>,
      #[serde(skip_serializing_if = "Option::is_none")]
      pub user_message: Option<String>,
      
      // Model selection (NEW)
      #[serde(skip_serializing_if = "Option::is_none")]
      pub model: Option<String>, // e.g., "xiaomi/mimo-v2-flash:free"
      
      // Existing attribution field
      pub attribution: Attribution,
  }
  ```

**C. Backward Compatibility:**
- [ ] `generate_context` command updated with payload sniffing:
  ```rust
  // If `messages` exists, use new format
  // Else convert: system_prompt + user_message â†’ messages array
  let messages = if let Some(msgs) = payload.messages {
      msgs
  } else {
      vec![
          Message { role: "system".to_string(), content: payload.system_prompt.unwrap_or_default(), name: None },
          Message { role: "user".to_string(), content: payload.user_message.unwrap_or_default(), name: None },
      ]
  };
  ```
- [ ] Existing `ContextPanel` calls (Story 3.4) continue working without changes

**D. Provider Trait Update:**
- [ ] `AiProvider` trait signature updated to accept new `ContextPayload`
- [ ] `OpenRouterProvider` respects `model` parameter from payload (overrides default)

### 3. Vercel AI SDK Integration (Frontend - NEW Client)

**Given** reasoning loops require Vercel AI SDK features (`maxSteps`, tool calling)
**When** creating the AI client
**Then** `src/lib/ai/client.ts` is created with:

**A. Custom Tauri Provider:**
- [ ] `createTauriLanguageModel()` function implemented:
  ```typescript
  // Implements Vercel AI SDK LanguageModelV1 interface
  // Bridges Vercel SDK â†’ Tauri backend commands
  function createTauriLanguageModel(config: {
    provider: string;     // From aiStore (e.g., "openrouter")
    model?: string;       // Optional model override
  }): LanguageModelV1
  ```
- [ ] Provider handles streaming via Tauri events (`ai-chunk`, `ai-inference-failed`)
- [ ] Provider converts Vercel SDK format â†’ backend `ContextPayload`

**B. maxSteps Support:**
- [ ] Client accepts `maxSteps` parameter:
  ```typescript
  interface ReasoningOptions {
    maxSteps?: number;  // Default: 0 (Flash mode), 10 (Thinking mode)
    tools?: Record<string, Tool>; // Vercel AI SDK tools
  }
  ```
- [ ] `maxSteps > 0` enables reasoning loops (Thinking mode)
- [ ] `maxSteps = 0` is single-shot inference (Flash mode)

**C. Model Selection:**
- [ ] Default model for Thinking mode: `xiaomi/mimo-v2-flash:free` (per Epic 4.5 research)
- [ ] Model configurable via `model` parameter in payload

### 4. Reasoning Store (Zustand)

**Given** reasoning state needs persistence across navigation
**When** implementing state management
**Then** `src/stores/reasoningStore.ts` is created with:

**A. State Schema:**
- [ ] Full state interface defined:
  ```typescript
  interface StepHistoryEntry {
    stepId: string;
    output: string;           // LLM text output for this step
    timestamp: number;        // Unix timestamp (ms)
    toolCallsMade?: string[]; // Optional: tool names called (e.g., ["read_file", "list_dir"])
  }

  interface ReasoningState {
    // Multi-project support (keyed by projectId)
    byProject: Record<string, {
      activeMode: AgentMode;
      activeProtocol: string | null;
      currentStepId: string | null;
      stepHistory: StepHistoryEntry[];
    }>;
  }
  ```

**B. Persistence:**
- [ ] Session-only persistence using `localStorage`
- [ ] State cleared on browser close (not permanent)
- [ ] Per-project isolation (state keyed by `projectId`)

**C. Actions:**
- [ ] `setMode(projectId, mode)` - Switch between Flash/Thinking
- [ ] `startProtocol(projectId, protocolId)` - Begin protocol execution
- [ ] `completeStep(projectId, stepId, output, toolCalls?)` - Mark step done
- [ ] `reset(projectId)` - Clear protocol state for project

### 5. Unified System Prompt

**Given** Ronin-Thinking needs combined Architect + Developer capabilities
**When** constructing the system prompt
**Then** `src/lib/ai/prompts/ronin-thinking.ts` is created using this structure:

**A. Extract from `_bmad/bmm/agents/architect.md`:**
- [ ] `<identity>` (Line 54): "Senior architect with expertise in distributed systems..."
- [ ] `<principles>` (Line 56): Full principles text about user-driven design, boring tech, etc.

**B. Extract from `_bmad/bmm/agents/dev.md`:**
- [ ] `<identity>` (Line 57): "Executes approved stories with strict adherence..."
- [ ] `<principles>` (Line 59): Full principles about story-driven development
- [ ] **Critical Implementation Rules** from `<activation>` steps #4-#13 (Lines 19-28):
  - "READ the entire story file BEFORE any implementation"
  - "Execute tasks/subtasks IN ORDER"
  - "Never proceed with failing tests"
  - "Write failing test first, then implementation"

**C. Merge Template:**
- [ ] Combined prompt structure:
  ```typescript
  export const RONIN_THINKING_PROMPT = `You are Ronin, an intelligent developer assistant that combines system-level architectural thinking with precise implementation execution.

  ## Your Identity
  You synthesize two complementary capabilities:
  
  **Architectural Thinking:**
  ${architectIdentity}
  ${architectPrinciples}
  
  **Implementation Excellence:**
  ${devIdentity}
  ${devPrinciples}
  
  **Critical Implementation Rules:**
  ${devActivationRules} // Steps #4-#13 from dev.md
  
  ## Context-Aware Reasoning
  - When analyzing project structure, git history, or design decisions: Apply architectural thinking to understand patterns and constraints.
  - When implementing features or fixing bugs: Apply developer rules (test-first, sequential task execution, never skip tests).
  - Use chain-of-thought reasoning to show your work.
  `;
  ```

**D. Token Optimization:**
- [ ] Extract ONLY essential content (skip menu system, workflow handlers from activation steps)
- [ ] Total prompt size: <2000 tokens (measured with `tiktoken`)

### 6. Tool Scaffolding (Mock Tools for Testing)

**Given** reasoning loops require tools but Story 4.5.2 implements real tools
**When** scaffolding tools for testing
**Then** mock tools are created:

**A. Tool Schema Definitions (src/lib/ai/tools/schemas.ts):**
- [ ] Vercel AI SDK tool schemas defined for:
  - `read_file`: `{ path: string }` â†’ returns file content
  - `list_dir`: `{ path: string }` â†’ returns file/folder list
  - `git_status`: `{}` â†’ returns branch, uncommitted files
  - `git_log`: `{ limit?: number }` â†’ returns recent commits

**B. Mock Tool Handlers (src/lib/ai/tools/mock/index.ts):**
- [ ] Mock implementations for testing:
  ```typescript
  export const mockTools = {
    read_file: async ({ path }: { path: string }) => {
      return `// Mock content for ${path}\nexport default function example() {}`;
    },
    list_dir: async ({ path }: { path: string }) => {
      return JSON.stringify(["README.md", "package.json", "src/"]);
    },
    git_status: async () => {
      return JSON.stringify({ branch: "main", uncommitted: 2 });
    },
    git_log: async ({ limit = 5 }) => {
      return JSON.stringify([
        { hash: "abc123", message: "feat: add feature X", date: "2025-12-20" }
      ]);
    }
  };
  ```

**C. Real Tool Implementation Deferred:**
- [ ] Real Tauri commands for tools implemented in Story 4.5.2
- [ ] Mock tools enable testing reasoning loops WITHOUT waiting for full tool implementation

### 7. Error Handling & Recovery

**Given** reasoning loops can fail in multiple ways
**When** handling errors
**Then** the following scenarios are covered:

- [ ] **Tool call errors:** Logged but don't crash reasoning loop (graceful degradation)
- [ ] **Max steps exhausted:** Display partial results with message: "Reasoning incomplete (max steps reached)"
- [ ] **Network failures:** Pause reasoning, allow resume from last completed step
- [ ] **Invalid tool calls:** Wrong parameters logged, error returned to LLM for retry
- [ ] **Model refuses tools:** If model generates text instead of calling tool, log warning and continue

### 8. Testability (Debug Page)

**Given** the agent core needs testing before UI is built
**When** creating verification infrastructure
**Then** `src/pages/DebugAgent.tsx` is created with:

**A. Test Interface:**
- [ ] Button: "Test Reasoning Loop (Hello World)"
- [ ] Button: "Test Multi-Step Reasoning"
- [ ] Button: "Test Max Steps Limit"
- [ ] Debug output panel showing:
  - Tool calls made
  - Step completions
  - Final response

**B. Test Cases:**

**Test 1: Single Tool Call**
- [ ] Prompt: "List files in the current directory"
- [ ] Expected: Model calls `list_dir` mock tool once
- [ ] Verify: Response includes mock file list (README.md, package.json, src/)

**Test 2: Multi-Step Reasoning** 
- [ ] Prompt: "What framework is this project using? Check package.json"
- [ ] Expected: Model calls `list_dir` â†’ `read_file(package.json)` â†’ synthesizes answer
- [ ] Verify: Answer mentions framework from mock package.json

**Test 3: Max Steps Limit**
- [ ] Set `maxSteps: 2`, send prompt requiring 3+ steps
- [ ] Expected: Graceful termination at step 2 with partial results
- [ ] Verify: Error message "Reasoning incomplete (max steps reached)"

**C. Integration Checkpoint:**
- [ ] All 3 tests pass before Story 4.5.2 begins
- [ ] Debug output visible in browser console
- [ ] Reasoning store state updates visible in React DevTools

## Tasks / Subtasks

### Backend Tasks

- [ ] **1. Define Message Struct & Update ContextPayload (src-tauri/src/ai/provider.rs)**
  - [ ] 1.1. Add `Message` struct with `role`, `content`, `name` fields
  - [ ] 1.2. Update `ContextPayload` to support both `messages` array AND legacy `system_prompt + user_message`
  - [ ] 1.3. Add optional `model` field to `ContextPayload`
  - [ ] 1.4. Write unit test: Serialize/deserialize both payload formats

- [ ] **2. Update AiProvider Trait (src-tauri/src/ai/provider.rs)**
  - [ ] 2.1. Update trait to accept new `ContextPayload` structure
  - [ ] 2.2. Update trait documentation with migration notes

- [ ] **3. Update OpenRouterProvider (src-tauri/src/ai/providers/openrouter.rs)**
  - [ ] 3.1. Accept new `ContextPayload` with messages array
  - [ ] 3.2. Convert messages array to OpenRouter API format
  - [ ] 3.3. Respect `model` parameter from payload (override default)
  - [ ] 3.4. Write integration test: Messages array â†’ OpenRouter API call

- [ ] **4. Update generate_context Command (src-tauri/src/commands/ai.rs)**
  - [ ] 4.1. Implement payload sniffing: Check if `messages` exists
  - [ ] 4.2. If `messages` missing, convert `system_prompt + user_message` â†’ messages array
  - [ ] 4.3. Add integration test: Old payload format still works (backward compatibility)
  - [ ] 4.4. Add integration test: New payload format with messages array
  - [ ] 4.5. Verify existing ContextPanel calls (Story 3.4) still work

### Frontend Tasks

- [ ] **5. Define Types & Interfaces (src/types/agent.ts - NEW)**
  - [ ] 5.1. Define `AgentMode` type
  - [ ] 5.2. Define `ReasoningProtocol` interface
  - [ ] 5.3. Define `ProtocolStep` interface
  - [ ] 5.4. Define `StepHistoryEntry` interface
  - [ ] 5.5. Import shared types from `src/types/ai.ts` where applicable

- [ ] **6. Implement Vercel AI SDK Client (src/lib/ai/client.ts - NEW)**
  - [ ] 6.1. Implement `createTauriLanguageModel` function (Vercel SDK `LanguageModelV1` interface)
  - [ ] 6.2. Implement streaming handler: Listen to `ai-chunk` Tauri events
  - [ ] 6.3. Implement error handler: Listen to `ai-inference-failed` events
  - [ ] 6.4. Add `maxSteps` parameter support
  - [ ] 6.5. Add tool calling support (tools passed to Vercel SDK `streamText`)
  - [ ] 6.6. Write unit test: Mock Tauri events â†’ Vercel SDK stream

- [ ] **7. Implement Reasoning Store (src/stores/reasoningStore.ts - NEW)**
  - [ ] 7.1. Define full `ReasoningState` interface with `byProject` structure
  - [ ] 7.2. Implement `setMode` action
  - [ ] 7.3. Implement `startProtocol` action
  - [ ] 7.4. Implement `completeStep` action
  - [ ] 7.5. Implement `reset` action
  - [ ] 7.6. Add `localStorage` persistence (session-only)
  - [ ] 7.7. Write unit test: Actions update state correctly
  - [ ] 7.8. Write unit test: State persists across page refresh (localStorage)

- [ ] **8. Construct System Prompt (src/lib/ai/prompts/ronin-thinking.ts - NEW)**
  - [ ] 8.1. Extract Architect `<identity>` and `<principles>` from `_bmad/bmm/agents/architect.md`
  - [ ] 8.2. Extract Developer `<identity>` and `<principles>` from `_bmad/bmm/agents/dev.md`
  - [ ] 8.3. Extract Developer activation rules (steps #4-#13) from dev.md
  - [ ] 8.4. Combine into unified prompt using template from AC #5.C
  - [ ] 8.5. Measure token count with `tiktoken` (target: <2000 tokens)
  - [ ] 8.6. Write unit test: Prompt includes all required sections

- [ ] **9. Scaffold Mock Tools (src/lib/ai/tools/)**
  - [ ] 9.1. Create `schemas.ts` with Vercel AI SDK tool schemas for 4 tools
  - [ ] 9.2. Create `mock/index.ts` with mock implementations
  - [ ] 9.3. Write unit test: Mock tools return expected format

- [ ] **10. Create Debug Page (src/pages/DebugAgent.tsx - NEW, Temporary)**
  - [ ] 10.1. Create basic UI with 3 test buttons (per AC #8.B)
  - [ ] 10.2. Implement "Test Reasoning Loop (Hello World)" test
  - [ ] 10.3. Implement "Test Multi-Step Reasoning" test
  - [ ] 10.4. Implement "Test Max Steps Limit" test
  - [ ] 10.5. Add debug output panel showing tool calls and responses
  - [ ] 10.6. Add route to `src/App.tsx` (temporary route: `/debug/agent`)

### Verification Tasks

- [ ] **11. Integration Testing**
  - [ ] 11.1. Run all 3 debug page tests, verify they pass
  - [ ] 11.2. Verify reasoning store state updates correctly in React DevTools
  - [ ] 11.3. Verify backward compatibility: Existing ContextPanel (Story 3.4) works
  - [ ] 11.4. Verify error handling: Network failure, max steps, tool errors

- [ ] **12. Documentation**
  - [ ] 12.1. Add inline comments documenting payload migration strategy
  - [ ] 12.2. Document mock tool scaffolding approach
  - [ ] 12.3. Add TODO comments noting Story 4.5.2 will implement real tools

## Dev Notes

### Backend Payload Migration Strategy

**Current State (Story 3.4):**
```rust
pub struct ContextPayload {
    pub system_prompt: String,
    pub user_message: String,
    pub attribution: Attribution,
}
```

**New State (This Story):**
```rust
pub struct ContextPayload {
    // NEW: Multi-turn support
    #[serde(skip_serializing_if = "Option::is_none")]
    pub messages: Option<Vec<Message>>,
    
    // LEGACY: Backward compatibility
    #[serde(skip_serializing_if = "Option::is_none")]
    pub system_prompt: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub user_message: Option<String>,
    
    // NEW: Model selection
    #[serde(skip_serializing_if = "Option::is_none")]
    pub model: Option<String>,
    
    pub attribution: Attribution,
}
```

**Migration Logic in `generate_context` Command:**
```rust
let messages = if let Some(msgs) = payload.messages {
    // New format: use messages directly
    msgs
} else {
    // Legacy format: convert to messages array
    vec![
        Message {
            role: "system".to_string(),
            content: payload.system_prompt.unwrap_or_default(),
            name: None,
        },
        Message {
            role: "user".to_string(),
            content: payload.user_message.unwrap_or_default(),
            name: None,
        },
    ]
};
```

**Critical:** This ensures existing ContextPanel calls (Story 3.4) continue working without ANY frontend changes.

### Vercel AI SDK Client Architecture

**Why Create Custom Tauri Provider?**
- Vercel AI SDK expects direct API calls (fetch to OpenAI/Anthropic endpoints)
- Ronin's architecture routes ALL API calls through Tauri backend (security, encryption, provider abstraction)
- Solution: Implement `LanguageModelV1` interface that bridges Vercel SDK â†’ Tauri backend

**Flow:**
```
Vercel SDK streamText({ model: customTauriModel })
  â†“
customTauriModel.doStream()
  â†“
invoke("generate_context", { payload })
  â†“
Listen to Tauri events: "ai-chunk"
  â†“
Yield chunks to Vercel SDK stream
  â†“
Vercel SDK handles tool calls, maxSteps logic
```

**Key Insight:** `maxSteps` logic runs on FRONTEND (Vercel SDK handles it). Backend remains stateless.

### System Prompt Construction Guide

**DO Extract:**
- `<identity>`: Core persona definition
- `<principles>`: Guiding principles for decision-making
- Developer activation rules: Test-first, sequential execution, never skip tests

**DO NOT Extract:**
- `<activation>` menu system logic (not relevant to Ronin-Thinking)
- Workflow handlers (XML runner logic)
- Config loading steps

**Token Budget:**
- Architect extraction: ~400 tokens
- Developer extraction: ~800 tokens
- Template overhead: ~200 tokens
- **Total target: <2000 tokens** (leaves headroom for context in reasoning)

### Model Selection Rationale

**Default: `xiaomi/mimo-v2-flash:free`**
- 309B parameters (MoE architecture)
- SWE-Bench Verified: 73.4% (excellent reasoning)
- Free tier available on OpenRouter
- Fast inference (<2s first token)

**Fallback: `z-ai/glm-4.5-air:free`**
- 106B parameters (MoE)
- Designed for agentic tasks
- Also free

**Premium (Optional): `anthropic/claude-3.5-sonnet`**
- Best-in-class for complex reasoning
- Requires paid API key
- User can configure in settings

### Tool Scaffolding Strategy

**Why Mock Tools in Story 4.5.1?**
- Reasoning loops REQUIRE tools to call (AC #6 requires "basic reasoning loop works")
- Real Tauri commands for file/git operations implemented in Story 4.5.2
- Mock tools enable testing agent core INDEPENDENTLY before full tool implementation

**Mock Tool Characteristics:**
- Return realistic JSON format
- Fast execution (no I/O)
- Deterministic responses (same input â†’ same output)
- Sufficient for testing reasoning loop logic

**Story 4.5.2 will:**
- Replace mocks with real Tauri commands
- Add file system access (`read_file`, `list_dir`)
- Add git operations (`git_status`, `git_log`)

### Integration Checkpoint

**Before Story 4.5.2 begins, verify:**
1. âœ… Debug page shows successful tool calls in console
2. âœ… Reasoning store updates visible in React DevTools
3. âœ… All 3 test cases pass (single tool, multi-step, max steps)
4. âœ… Existing Story 3.4 ContextPanel functionality unaffected

**This proves:**
- Backend payload migration works
- Vercel SDK client bridges to Tauri correctly
- Tool calling infrastructure works end-to-end
- Ready to implement real tools in Story 4.5.2

### Future Integration Note (Story 4.5.3)

**Agent Route Decision:**
- Story 4.5.3 will create dedicated `/agent/:projectId` route
- This story's debug page is TEMPORARY
- **Do NOT invest in polished UI** for debug page
- Store state in `reasoningStore` structured for future route consumption

### Attribution Extension (Future)

**Current Attribution (Story 3.4):**
```
Based on: âˆž 10 commits Â· ðŸ“„ 5 files Â· ðŸ“ DEVLOG
```

**Future Attribution (After Story 4.5.2):**
```
Based on: âˆž 10 commits Â· ðŸ“„ 5 files Â· ðŸ“ DEVLOG Â· ðŸ”§ read_fileÃ—3 git_logÃ—1
```

**Implementation (Deferred to Story 4.5.2):**
Extend `Attribution` interface:
```typescript
interface Attribution {
  commits: number;
  files: number;
  sources: string[];
  devlogLines?: number;
  toolCalls?: Array<{ tool: string; count: number }>; // NEW
}
```

### Files to Create/Modify

**Backend (Rust):**
- `src-tauri/src/ai/provider.rs` (Modify - Add `Message` struct, update `ContextPayload`)
- `src-tauri/src/ai/providers/openrouter.rs` (Modify - Accept new payload, respect model param)
- `src-tauri/src/commands/ai.rs` (Modify - Add payload sniffing for backward compatibility)

**Frontend (TypeScript):**
- `src/types/agent.ts` (NEW - AgentMode, ReasoningProtocol, ProtocolStep, StepHistoryEntry)
- `src/lib/ai/client.ts` (NEW - Vercel SDK client with Tauri bridge)
- `src/lib/ai/tools/schemas.ts` (NEW - Tool schemas for Vercel SDK)
- `src/lib/ai/tools/mock/index.ts` (NEW - Mock tool implementations)
- `src/stores/reasoningStore.ts` (NEW - Zustand store for reasoning state)
- `src/lib/ai/prompts/ronin-thinking.ts` (NEW - Unified system prompt)
- `src/pages/DebugAgent.tsx` (NEW - Temporary debug page)
- `src/App.tsx` (Modify - Add debug route)

## Technical Requirements

### Performance Targets

| Metric | Target | Notes |
|--------|--------|-------|
| First reasoning chunk | <3s | Includes LLM inference start |
| Tool call execution (mock) | <50ms | Mock tools are in-memory |
| Reasoning loop (3 steps) | <15s | With MiMo-V2-Flash model |
| Store state update | <10ms | Zustand local state |

### maxSteps Behavior

**How maxSteps Works (Vercel AI SDK):**
1. LLM generates response
2. If response contains tool call â†’ Execute tool â†’ Add to messages â†’ Call LLM again
3. Repeat until:
   - LLM stops calling tools (natural completion), OR
   - `maxSteps` reached (forced termination), OR
   - Error occurs (graceful degradation)

**Configuration:**
- **Flash mode:** `maxSteps: 0` (single-shot, no tools)
- **Thinking mode:** `maxSteps: 10` (multi-turn reasoning)

**Termination Behavior:**
- Natural completion: Return final response
- Max steps reached: Return partial response + warning message
- Error: Return error + outputs from completed steps (recovery)

## Definition of Done

- [ ] **All Tasks Complete:** 12 task sections with 60+ subtasks marked as `[x]`
- [ ] **Backend Tests Pass:**
  - [ ] Unit tests for `Message` struct and `ContextPayload` serialization
  - [ ] Integration test: Old payload format (backward compatibility)
  - [ ] Integration test: New payload format with messages array
  - [ ] All existing tests pass (no regressions)
- [ ] **Frontend Tests Pass:**
  - [ ] Unit tests for reasoning store actions
  - [ ] Unit tests for mock tools
  - [ ] Integration tests: All 3 debug page tests pass
- [ ] **Debug Page Verification:**
  - [ ] Test 1: Single tool call works
  - [ ] Test 2: Multi-step reasoning works
  - [ ] Test 3: Max steps limit enforced correctly
- [ ] **Backward Compatibility Verified:**
  - [ ] Story 3.4 ContextPanel functionality works without changes
  - [ ] No breaking changes to existing Tauri commands
- [ ] **Documentation Complete:**
  - [ ] Inline comments explain payload migration
  - [ ] Mock tool scaffolding documented
  - [ ] TODOs added for Story 4.5.2 real tool implementation
- [ ] **Ready for Story 4.5.2:**
  - [ ] Tool schemas defined (real implementation can replace mocks)
  - [ ] Reasoning store ready for real protocol execution
  - [ ] Integration checkpoint tests all pass

## Dev Agent Record

### Agent Model Used
{{agent_model_name_version}}

### Debug Log References

### Completion Notes List

### File List

