# Story 4.5.1: Agent Core & System Prompt

Status: review

<!-- Validation completed: 2025-12-22 - All critical improvements applied -->

## Dependencies

**Requires (must be completed first):**
- Story 4.25.1: Unified API Client (backend `AiProvider` trait created, but NO Vercel SDK client yet)
- Story 3.4: AI Context Generation (existing streaming infrastructure)

**Key Dependency Note:**
- Story 4.25.1 created backend provider abstraction but **DID NOT** create `src/lib/ai/client.ts`
- This story creates the **FIRST** Vercel AI SDK runtime integration (not extending existing client)
- Backend currently supports only single `system_prompt + user_message` â†’ This story adds `messages` array support

**Blocks (cannot start until this is done):**
- Story 4.5.2: Tool Implementation & Protocol Execution (requires agent core and mock tools from this story)

## Story

As a **developer**,
I want **the agent core with unified system prompt, tool scaffolding, and Vercel AI SDK client**,
So that **the reasoning infrastructure can be tested with mock tools before building full tool implementations and UI**.

## Acceptance Criteria

### 1. Types & Interfaces

**Given** the reasoning infrastructure needs type safety
**When** defining core types
**Then** the following TypeScript types are created in `src/types/agent.ts`:
- [x] `AgentMode` type: `"ronin-flash" | "ronin-thinking"`
- [x] `ReasoningProtocol` interface with:
  ```typescript
  interface ReasoningProtocol {
    id: string;               // e.g., "project-resurrection"
    title: string;
    description: string;
    steps: ProtocolStep[];
  }
  ```
- [x] `ProtocolStep` interface with:
  ```typescript
  interface ProtocolStep {
    id: string;               // e.g., "step-01-map-structure"
    title: string;
    instruction: string;      // LLM instruction for this step
    requiredOutput: 'file_creation' | 'user_confirmation' | 'none';
  }
  ```

### 2. Backend AI Infrastructure (Breaking Change - Messages Array)

**Given** reasoning loops require multi-turn conversation support
**When** upgrading the backend
**Then** the following changes are implemented:

**A. Message Struct Definition (src-tauri/src/ai/provider.rs):**
- [x] `Message` struct defined:
  ```rust
  #[derive(Debug, Clone, Serialize, Deserialize)]
  pub struct Message {
      pub role: String,    // "system", "user", "assistant", "tool"
      pub content: String,
      #[serde(skip_serializing_if = "Option::is_none")]
      pub name: Option<String>, // For tool calls
  }
  ```

**B. Payload Update:**
- [x] `ContextPayload` struct updated to support BOTH formats:
  ```rust
  #[derive(Debug, Clone, Serialize, Deserialize)]
  pub struct ContextPayload {
      // NEW: Multi-turn conversation support
      #[serde(skip_serializing_if = "Option::is_none")]
      pub messages: Option<Vec<Message>>,
      
      // LEGACY: Single-turn support (Story 3.4 compatibility)
      #[serde(skip_serializing_if = "Option::is_none")]
      pub system_prompt: Option<String>,
      #[serde(skip_serializing_if = "Option::is_none")]
      pub user_message: Option<String>,
      
      // Model selection (NEW)
      #[serde(skip_serializing_if = "Option::is_none")]
      pub model: Option<String>, // e.g., "xiaomi/mimo-v2-flash:free"
      
      // Existing attribution field
      pub attribution: Attribution,
  }
  ```

**C. Backward Compatibility:**
- [x] `generate_context` command updated with payload sniffing:
  ```rust
  // If `messages` exists, use new format
  // Else convert: system_prompt + user_message â†’ messages array
  let messages = if let Some(msgs) = payload.messages {
      msgs
  } else {
      vec![
          Message { role: "system".to_string(), content: payload.system_prompt.unwrap_or_default(), name: None },
          Message { role: "user".to_string(), content: payload.user_message.unwrap_or_default(), name: None },
      ]
  };
  ```
- [x] Existing `ContextPanel` calls (Story 3.4) continue working without changes

**D. Provider Trait Update:**
- [x] `AiProvider` trait signature updated to accept new `ContextPayload`
- [x] `OpenRouterProvider` respects `model` parameter from payload (overrides default)

### 3. Vercel AI SDK Integration (Frontend - NEW Client)

**Given** reasoning loops require Vercel AI SDK features (`maxSteps`, tool calling)
**When** creating the AI client
**Then** `src/lib/ai/client.ts` is created with:

**A. Custom Tauri Provider:**
- [x] `createTauriLanguageModel()` function implemented:
  ```typescript
  // Implements Vercel AI SDK LanguageModelV1 interface
  // Bridges Vercel SDK â†’ Tauri backend commands
  function createTauriLanguageModel(config: {
    provider: string;     // From aiStore (e.g., "openrouter")
    model?: string;       // Optional model override
  }): LanguageModelV1
  ```
- [x] Provider handles streaming via Tauri events (`ai-chunk`, `ai-inference-failed`)
- [x] Provider converts Vercel SDK format â†’ backend `ContextPayload`

**B. maxSteps Support:**
- [x] Client accepts `maxSteps` parameter:
  ```typescript
  interface ReasoningOptions {
    maxSteps?: number;  // Default: 0 (Flash mode), 10 (Thinking mode)
    tools?: Record<string, Tool>; // Vercel AI SDK tools
  }
  ```
- [x] `maxSteps > 0` enables reasoning loops (Thinking mode)
- [x] `maxSteps = 0` is single-shot inference (Flash mode)

**C. Model Selection:**
- [x] Default model for Thinking mode: `xiaomi/mimo-v2-flash:free` (per Epic 4.5 research)
- [x] Model configurable via `model` parameter in payload

### 4. Reasoning Store (Zustand)

**Given** reasoning state needs persistence across navigation
**When** implementing state management
**Then** `src/stores/reasoningStore.ts` is created with:

**A. State Schema:**
- [x] Full state interface defined:
  ```typescript
  interface StepHistoryEntry {
    stepId: string;
    output: string;           // LLM text output for this step
    timestamp: number;        // Unix timestamp (ms)
    toolCallsMade?: string[]; // Optional: tool names called (e.g., ["read_file", "list_dir"])
  }

  interface ReasoningState {
    // Multi-project support (keyed by projectId)
    byProject: Record<string, {
      activeMode: AgentMode;
      activeProtocol: string | null;
      currentStepId: string | null;
      stepHistory: StepHistoryEntry[];
    }>;
  }
  ```

**B. Persistence:**
- [x] Session-only persistence using `localStorage`
- [x] State cleared on browser close (not permanent)
- [x] Per-project isolation (state keyed by `projectId`)

**C. Actions:**
- [x] `setMode(projectId, mode)` - Switch between Flash/Thinking
- [x] `startProtocol(projectId, protocolId)` - Begin protocol execution
- [x] `completeStep(projectId, stepId, output, toolCalls?)` - Mark step done
- [x] `reset(projectId)` - Clear protocol state for project

### 5. Unified System Prompt

**Given** Ronin-Thinking needs combined Architect + Developer capabilities
**When** constructing the system prompt
**Then** `src/lib/ai/prompts/ronin-thinking.ts` is created using this structure:

**A. Extract from `_bmad/bmm/agents/architect.md`:**
- [x] `<identity>` (Line 54): "Senior architect with expertise in distributed systems..."
- [x] `<principles>` (Line 56): Full principles text about user-driven design, boring tech, etc.

**B. Extract from `_bmad/bmm/agents/dev.md`:**
- [x] `<identity>` (Line 57): "Executes approved stories with strict adherence..."
- [x] `<principles>` (Line 59): Full principles about story-driven development
- [x] **Critical Implementation Rules** from `<activation>` steps #4-#13 (Lines 19-28):
  - "READ the entire story file BEFORE any implementation"
  - "Execute tasks/subtasks IN ORDER"
  - "Never proceed with failing tests"
  - "Write failing test first, then implementation"

**C. Merge Template:**
- [x] Combined prompt structure:
  ```typescript
  export const RONIN_THINKING_PROMPT = `You are Ronin, an intelligent developer assistant that combines system-level architectural thinking with precise implementation execution.

  ## Your Identity
  You synthesize two complementary capabilities:
  
  **Architectural Thinking:**
  ${architectIdentity}
  ${architectPrinciples}
  
  **Implementation Excellence:**
  ${devIdentity}
  ${devPrinciples}
  
  **Critical Implementation Rules:**
  ${devActivationRules} // Steps #4-#13 from dev.md
  
  ## Context-Aware Reasoning
  - When analyzing project structure, git history, or design decisions: Apply architectural thinking to understand patterns and constraints.
  - When implementing features or fixing bugs: Apply developer rules (test-first, sequential task execution, never skip tests).
  - Use chain-of-thought reasoning to show your work.
  `;
  ```

**D. Token Optimization:**
- [x] Extract ONLY essential content (skip menu system, workflow handlers from activation steps)
- [x] Total prompt size: <2000 tokens (measured with `tiktoken`)

### 6. Tool Scaffolding (Mock Tools for Testing)

**Given** reasoning loops require tools but Story 4.5.2 implements real tools
**When** scaffolding tools for testing
**Then** mock tools are created:

**A. Tool Schema Definitions (src/lib/ai/tools/schemas.ts):**
- [x] Vercel AI SDK tool schemas defined for:
  - `read_file`: `{ path: string }` â†’ returns file content
  - `list_dir`: `{ path: string }` â†’ returns file/folder list
  - `git_status`: `{}` â†’ returns branch, uncommitted files
  - `git_log`: `{ limit?: number }` â†’ returns recent commits

**B. Mock Tool Handlers (src/lib/ai/tools/mock/index.ts):**
- [x] Mock implementations for testing:
  ```typescript
  export const mockTools = {
    read_file: async ({ path }: { path: string }) => {
      return `// Mock content for ${path}\nexport default function example() {}`;
    },
    list_dir: async ({ path }: { path: string }) => {
      return JSON.stringify(["README.md", "package.json", "src/"]);
    },
    git_status: async () => {
      return JSON.stringify({ branch: "main", uncommitted: 2 });
    },
    git_log: async ({ limit = 5 }) => {
      return JSON.stringify([
        { hash: "abc123", message: "feat: add feature X", date: "2025-12-20" }
      ]);
    }
  };
  ```

**C. Real Tool Implementation Deferred:**
- [x] Real Tauri commands for tools implemented in Story 4.5.2
- [x] Mock tools enable testing reasoning loops WITHOUT waiting for full tool implementation

### 7. Error Handling & Recovery

**Given** reasoning loops can fail in multiple ways
**When** handling errors
**Then** the following scenarios are covered:

- [x] **Tool call errors:** Logged but don't crash reasoning loop (graceful degradation)
- [x] **Max steps exhausted:** Display partial results with message: "Reasoning incomplete (max steps reached)"
- [x] **Network failures:** Pause reasoning, allow resume from last completed step
- [x] **Invalid tool calls:** Wrong parameters logged, error returned to LLM for retry
- [x] **Model refuses tools:** If model generates text instead of calling tool, log warning and continue

### 8. Testability (Debug Page)

**Given** the agent core needs testing before UI is built
**When** creating verification infrastructure
**Then** `src/pages/DebugAgent.tsx` is created with:

**A. Test Interface:**
- [x] Button: "Test Reasoning Loop (Hello World)"
- [x] Button: "Test Multi-Step Reasoning"
- [x] Button: "Test Max Steps Limit"
- [x] Debug output panel showing:
  - Tool calls made
  - Step completions
  - Final response

**B. Test Cases:**

**Test 1: Single Tool Call**
- [x] Prompt: "List files in the current directory"
- [x] Expected: Model calls `list_dir` mock tool once
- [x] Verify: Response includes mock file list (README.md, package.json, src/)

**Test 2: Multi-Step Reasoning** 
- [x] Prompt: "What framework is this project using? Check package.json"
- [x] Expected: Model calls `list_dir` â†’ `read_file(package.json)` â†’ synthesizes answer
- [x] Verify: Answer mentions framework from mock package.json

**Test 3: Max Steps Limit**
- [x] Set `maxSteps: 2`, send prompt requiring 3+ steps
- [x] Expected: Graceful termination at step 2 with partial results
- [x] Verify: Error message "Reasoning incomplete (max steps reached)"

**C. Integration Checkpoint:**
- [x] All 3 tests pass before Story 4.5.2 begins
- [x] Debug output visible in browser console
- [x] Reasoning store state updates visible in React DevTools

## Tasks / Subtasks

### Backend Tasks

- [x] **1. Define Message Struct & Update ContextPayload (src-tauri/src/ai/provider.rs)**
  - [x] 1.1. Add `Message` struct with `role`, `content`, `name` fields
  - [x] 1.2. Update `ContextPayload` to support both `messages` array AND legacy `system_prompt + user_message`
  - [x] 1.3. Add optional `model` field to `ContextPayload`
  - [x] 1.4. Write unit test: Serialize/deserialize both payload formats

- [x] **2. Update AiProvider Trait (src-tauri/src/ai/provider.rs)**
  - [x] 2.1. Update trait to accept new `ContextPayload` structure
  - [x] 2.2. Update trait documentation with migration notes

- [x] **3. Update OpenRouterProvider (src-tauri/src/ai/providers/openrouter.rs)**
  - [x] 3.1. Accept new `ContextPayload` with messages array
  - [x] 3.2. Convert messages array to OpenRouter API format
  - [x] 3.3. Respect `model` parameter from payload (override default)
  - [x] 3.4. Write integration test: Messages array â†’ OpenRouter API call

- [x] **4. Update generate_context Command (src-tauri/src/commands/ai.rs)**
  - [x] 4.1. Implement payload sniffing: Check if `messages` exists
  - [x] 4.2. If `messages` missing, convert `system_prompt + user_message` â†’ messages array
  - [x] 4.3. Add integration test: Old payload format still works (backward compatibility)
  - [x] 4.4. Add integration test: New payload format with messages array
  - [x] 4.5. Verify existing ContextPanel calls (Story 3.4) still work

### Frontend Tasks

- [x] **5. Define Types & Interfaces (src/types/agent.ts - NEW)**
  - [x] 5.1. Define `AgentMode` type
  - [x] 5.2. Define `ReasoningProtocol` interface
  - [x] 5.3. Define `ProtocolStep` interface
  - [x] 5.4. Define `StepHistoryEntry` interface
  - [x] 5.5. Import shared types from `src/types/ai.ts` where applicable

- [x] **6. Implement Vercel AI SDK Client (src/lib/ai/client.ts - NEW)**
  - [x] 6.1. Implement `createTauriLanguageModel` function (Vercel SDK `LanguageModelV1` interface)
  - [x] 6.2. Implement streaming handler: Listen to `ai-chunk` Tauri events
  - [x] 6.3. Implement error handler: Listen to `ai-inference-failed` events
  - [x] 6.4. Add `maxSteps` parameter support
  - [x] 6.5. Add tool calling support (tools passed to Vercel SDK `streamText`)
  - [x] 6.6. Write unit test: Mock Tauri events â†’ Vercel SDK stream

- [x] **7. Implement Reasoning Store (src/stores/reasoningStore.ts - NEW)**
  - [x] 7.1. Define full `ReasoningState` interface with `byProject` structure
  - [x] 7.2. Implement `setMode` action
  - [x] 7.3. Implement `startProtocol` action
  - [x] 7.4. Implement `completeStep` action
  - [x] 7.5. Implement `reset` action
  - [x] 7.6. Add `localStorage` persistence (session-only)
  - [x] 7.7. Write unit test: Actions update state correctly
  - [x] 7.8. Write unit test: State persists across page refresh (localStorage)

- [x] **8. Construct System Prompt (src/lib/ai/prompts/ronin-thinking.ts - NEW)**
  - [x] 8.1. Extract Architect `<identity>` and `<principles>` from `_bmad/bmm/agents/architect.md`
  - [x] 8.2. Extract Developer `<identity>` and `<principles>` from `_bmad/bmm/agents/dev.md`
  - [x] 8.3. Extract Developer activation rules (steps #4-#13) from dev.md
  - [x] 8.4. Combine into unified prompt using template from AC #5.C
  - [x] 8.5. Measure token count with `tiktoken` (target: <2000 tokens)
  - [x] 8.6. Write unit test: Prompt includes all required sections

- [x] **9. Scaffold Mock Tools (src/lib/ai/tools/)**
  - [x] 9.1. Create `schemas.ts` with Vercel AI SDK tool schemas for 4 tools
  - [x] 9.2. Create `mock/index.ts` with mock implementations
  - [x] 9.3. Write unit test: Mock tools return expected format

- [x] **10. Create Debug Page (src/pages/DebugAgent.tsx - NEW, Temporary)**
  - [x] 10.1. Create basic UI with 3 test buttons (per AC #8.B)
  - [x] 10.2. Implement "Test Reasoning Loop (Hello World)" test
  - [x] 10.3. Implement "Test Multi-Step Reasoning" test
  - [x] 10.4. Implement "Test Max Steps Limit" test
  - [x] 10.5. Add debug output panel showing tool calls and responses
  - [x] 10.6. Add route to `src/App.tsx` (temporary route: `/debug/agent`)

### Verification Tasks

- [x] **11. Integration Testing**
  - [x] 11.1. Run all 3 debug page tests, verify they pass
  - [x] 11.2. Verify reasoning store state updates correctly in React DevTools
  - [x] 11.3. Verify backward compatibility: Existing ContextPanel (Story 3.4) works
  - [x] 11.4. Verify error handling: Network failure, max steps, tool errors

- [x] **12. Documentation**
  - [x] 12.1. Add inline comments documenting payload migration strategy
  - [x] 12.2. Document mock tool scaffolding approach
  - [x] 12.3. Add TODO comments noting Story 4.5.2 will implement real tools

## Dev Notes

### Backend Payload Migration Strategy

**Current State (Story 3.4):**
```rust
pub struct ContextPayload {
    pub system_prompt: String,
    pub user_message: String,
    pub attribution: Attribution,
}
```

**New State (This Story):**
```rust
pub struct ContextPayload {
    // NEW: Multi-turn support
    #[serde(skip_serializing_if = "Option::is_none")]
    pub messages: Option<Vec<Message>>,
    
    // LEGACY: Backward compatibility
    #[serde(skip_serializing_if = "Option::is_none")]
    pub system_prompt: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub user_message: Option<String>,
    
    // NEW: Model selection
    #[serde(skip_serializing_if = "Option::is_none")]
    pub model: Option<String>,
    
    pub attribution: Attribution,
}
```

**Migration Logic in `generate_context` Command:**
```rust
let messages = if let Some(msgs) = payload.messages {
    // New format: use messages directly
    msgs
} else {
    // Legacy format: convert to messages array
    vec![
        Message {
            role: "system".to_string(),
            content: payload.system_prompt.unwrap_or_default(),
            name: None,
        },
        Message {
            role: "user".to_string(),
            content: payload.user_message.unwrap_or_default(),
            name: None,
        },
    ]
};
```

**Critical:** This ensures existing ContextPanel calls (Story 3.4) continue working without ANY frontend changes.

### Vercel AI SDK Client Architecture

**Why Create Custom Tauri Provider?**
- Vercel AI SDK expects direct API calls (fetch to OpenAI/Anthropic endpoints)
- Ronin's architecture routes ALL API calls through Tauri backend (security, encryption, provider abstraction)
- Solution: Implement `LanguageModelV1` interface that bridges Vercel SDK â†’ Tauri backend

**Flow:**
```
Vercel SDK streamText({ model: customTauriModel })
  â†“
customTauriModel.doStream()
  â†“
invoke("generate_context", { payload })
  â†“
Listen to Tauri events: "ai-chunk"
  â†“
Yield chunks to Vercel SDK stream
  â†“
Vercel SDK handles tool calls, maxSteps logic
```

**Key Insight:** `maxSteps` logic runs on FRONTEND (Vercel SDK handles it). Backend remains stateless.

### System Prompt Construction Guide

**DO Extract:**
- `<identity>`: Core persona definition
- `<principles>`: Guiding principles for decision-making
- Developer activation rules: Test-first, sequential execution, never skip tests

**DO NOT Extract:**
- `<activation>` menu system logic (not relevant to Ronin-Thinking)
- Workflow handlers (XML runner logic)
- Config loading steps

**Token Budget:**
- Architect extraction: ~400 tokens
- Developer extraction: ~800 tokens
- Template overhead: ~200 tokens
- **Total target: <2000 tokens** (leaves headroom for context in reasoning)

### Model Selection Rationale

**Default: `xiaomi/mimo-v2-flash:free`**
- 309B parameters (MoE architecture)
- SWE-Bench Verified: 73.4% (excellent reasoning)
- Free tier available on OpenRouter
- Fast inference (<2s first token)

**Fallback: `z-ai/glm-4.5-air:free`**
- 106B parameters (MoE)
- Designed for agentic tasks
- Also free

**Premium (Optional): `anthropic/claude-3.5-sonnet`**
- Best-in-class for complex reasoning
- Requires paid API key
- User can configure in settings

### Tool Scaffolding Strategy

**Why Mock Tools in Story 4.5.1?**
- Reasoning loops REQUIRE tools to call (AC #6 requires "basic reasoning loop works")
- Real Tauri commands for file/git operations implemented in Story 4.5.2
- Mock tools enable testing agent core INDEPENDENTLY before full tool implementation

**Mock Tool Characteristics:**
- Return realistic JSON format
- Fast execution (no I/O)
- Deterministic responses (same input â†’ same output)
- Sufficient for testing reasoning loop logic

**Story 4.5.2 will:**
- Replace mocks with real Tauri commands
- Add file system access (`read_file`, `list_dir`)
- Add git operations (`git_status`, `git_log`)

### Integration Checkpoint

**Before Story 4.5.2 begins, verify:**
1. âœ… Debug page shows successful tool calls in console
2. âœ… Reasoning store updates visible in React DevTools
3. âœ… All 3 test cases pass (single tool, multi-step, max steps)
4. âœ… Existing Story 3.4 ContextPanel functionality unaffected

**This proves:**
- Backend payload migration works
- Vercel SDK client bridges to Tauri correctly
- Tool calling infrastructure works end-to-end
- Ready to implement real tools in Story 4.5.2

### Future Integration Note (Story 4.5.3)

**Agent Route Decision:**
- Story 4.5.3 will create dedicated `/agent/:projectId` route
- This story's debug page is TEMPORARY
- **Do NOT invest in polished UI** for debug page
- Store state in `reasoningStore` structured for future route consumption

### Attribution Extension (Future)

**Current Attribution (Story 3.4):**
```
Based on: âˆž 10 commits Â· ðŸ“„ 5 files Â· ðŸ“ DEVLOG
```

**Future Attribution (After Story 4.5.2):**
```
Based on: âˆž 10 commits Â· ðŸ“„ 5 files Â· ðŸ“ DEVLOG Â· ðŸ”§ read_fileÃ—3 git_logÃ—1
```

**Implementation (Deferred to Story 4.5.2):**
Extend `Attribution` interface:
```typescript
interface Attribution {
  commits: number;
  files: number;
  sources: string[];
  devlogLines?: number;
  toolCalls?: Array<{ tool: string; count: number }>; // NEW
}
```

### Files to Create/Modify

**Backend (Rust):**
- `src-tauri/src/ai/provider.rs` (Modify - Add `Message` struct, update `ContextPayload`)
- `src-tauri/src/ai/providers/openrouter.rs` (Modify - Accept new payload, respect model param)
- `src-tauri/src/commands/ai.rs` (Modify - Add payload sniffing for backward compatibility)

**Frontend (TypeScript):**
- `src/types/agent.ts` (NEW - AgentMode, ReasoningProtocol, ProtocolStep, StepHistoryEntry)
- `src/lib/ai/client.ts` (NEW - Vercel SDK client with Tauri bridge)
- `src/lib/ai/tools/schemas.ts` (NEW - Tool schemas for Vercel SDK)
- `src/lib/ai/tools/mock/index.ts` (NEW - Mock tool implementations)
- `src/stores/reasoningStore.ts` (NEW - Zustand store for reasoning state)
- `src/lib/ai/prompts/ronin-thinking.ts` (NEW - Unified system prompt)
- `src/pages/DebugAgent.tsx` (NEW - Temporary debug page)
- `src/App.tsx` (Modify - Add debug route)

## Technical Requirements

### Performance Targets

| Metric | Target | Notes |
|--------|--------|-------|
| First reasoning chunk | <3s | Includes LLM inference start |
| Tool call execution (mock) | <50ms | Mock tools are in-memory |
| Reasoning loop (3 steps) | <15s | With MiMo-V2-Flash model |
| Store state update | <10ms | Zustand local state |

### maxSteps Behavior

**How maxSteps Works (Vercel AI SDK):**
1. LLM generates response
2. If response contains tool call â†’ Execute tool â†’ Add to messages â†’ Call LLM again
3. Repeat until:
   - LLM stops calling tools (natural completion), OR
   - `maxSteps` reached (forced termination), OR
   - Error occurs (graceful degradation)

**Configuration:**
- **Flash mode:** `maxSteps: 0` (single-shot, no tools)
- **Thinking mode:** `maxSteps: 10` (multi-turn reasoning)

**Termination Behavior:**
- Natural completion: Return final response
- Max steps reached: Return partial response + warning message
- Error: Return error + outputs from completed steps (recovery)

## Definition of Done

- [x] **All Tasks Complete:** 12 task sections with 60+ subtasks marked as `[x]`
- [x] **Backend Tests Pass:**
  - [x] Unit tests for `Message` struct and `ContextPayload` serialization
  - [x] Integration test: Old payload format (backward compatibility)
  - [x] Integration test: New payload format with messages array
  - [x] All existing tests pass (no regressions)
- [x] **Frontend Tests Pass:**
  - [x] Unit tests for reasoning store actions
  - [x] Unit tests for mock tools
  - [x] Integration tests: All 3 debug page tests pass
- [x] **Debug Page Verification:**
  - [x] Test 1: Single tool call works
  - [x] Test 2: Multi-step reasoning works
  - [x] Test 3: Max steps limit enforced correctly
- [x] **Backward Compatibility Verified:**
  - [x] Story 3.4 ContextPanel functionality works without changes
  - [x] No breaking changes to existing Tauri commands
- [x] **Documentation Complete:**
  - [x] Inline comments explain payload migration
  - [x] Mock tool scaffolding documented
  - [x] TODOs added for Story 4.5.2 real tool implementation
- [x] **Ready for Story 4.5.2:**
  - [x] Tool schemas defined (real implementation can replace mocks)
  - [x] Reasoning store ready for real protocol execution
  - [x] Integration checkpoint tests all pass

## Dev Agent Record

### Agent Model Used
{{agent_model_name_version}}

### Debug Log References

### Completion Notes List

- 2025-12-22: Completed Backend Tasks 1-4.
  - Implemented `Message` struct and updated `ContextPayload` in `src-tauri/src/ai/provider.rs` to support multi-turn messages and model selection.
  - Updated `OpenRouterProvider` and `DemoProvider` to handle new payload format.
  - Refactored `generate_context` in `src-tauri/src/commands/ai.rs` to support optional frontend-provided payload while maintaining backward compatibility for legacy callers.
  - Verified backend changes with unit and integration tests (121 tests passed).

- 2025-12-22: Code Review Fixes Applied.
  - Fixed TypeScript errors in `DebugAgent.tsx` and `agent.ts` (unused imports, implicit any).
  - Created missing `schemas.ts` with all 4 tool schemas (read_file, list_dir, git_status, git_log).
  - Moved mock tools to correct directory structure (`tools/mock/index.ts`).
  - Updated system prompt to properly extract from BMAD agent files (architect.md, dev.md).
  - Fixed `createTauriLanguageModel` type documentation.
  - Guarded `/debug/agent` route for dev-only access.
  - Updated mock tools to use AI SDK v5 `inputSchema` format.
  - Updated File List to include all changed files.

### File List

#### Backend (Rust)
#### [MODIFY] src-tauri/src/ai/provider.rs
#### [MODIFY] src-tauri/src/ai/providers/openrouter.rs
#### [MODIFY] src-tauri/src/ai/providers/demo.rs
#### [MODIFY] src-tauri/src/commands/ai.rs
#### [MODIFY] src-tauri/src/commands/settings.rs

#### Frontend (TypeScript)
#### [NEW] src/types/agent.ts
#### [MODIFY] src/types/ai.ts
#### [NEW] src/lib/ai/client.ts
#### [NEW] src/stores/reasoningStore.ts
#### [NEW] src/lib/ai/prompts/ronin-thinking.ts
#### [NEW] src/lib/ai/tools/index.ts
#### [NEW] src/lib/ai/tools/schemas.ts
#### [NEW] src/lib/ai/tools/mock/index.ts
#### [NEW] src/pages/DebugAgent.tsx
#### [MODIFY] src/App.tsx

#### Config
#### [MODIFY] package.json
#### [MODIFY] package-lock.json

#### Sprint
#### [MODIFY] docs/sprint-artifacts/sprint-status.yaml
